{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI8akfjtfdc2"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit\n",
        "!pip install networkx\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxwh_o9sVG2h",
        "outputId": "629e7ce8-000e-4f8b-af48-f5fa45dc9f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzcqhabKfHlq",
        "outputId": "83413c3b-9b72-4668-80ae-124d08e33144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "model_name = \"xlm-roberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "path = f\"/content/drive/MyDrive/QA_System/Models/ROBERTA/xlm_roberta_large5.pt\"\n",
        "\n",
        "model=torch.load(path,map_location=torch.device('cpu'))\n",
        "model.eval()\n",
        "\n",
        "# Function to predict answer based on provided context and query\n",
        "def predict(context,query):\n",
        "\n",
        "  # Encode query and context \n",
        "  inputs = tokenizer.encode_plus(query, context, padding=True, max_length= 512, truncation=True, add_special_tokens = True, return_tensors='pt')\n",
        "\n",
        "  # Generate output\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "  # Extract start and end index \n",
        "  answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
        "  answer_end = torch.argmax(outputs[1]) + 1 \n",
        "\n",
        "  # Decode answer from tokenized form\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "\n",
        "  return answer\n",
        "\n",
        "# Normalize text -> Remove articles, spaces, punctuation marks and conversion to all upper cases to lower cases\n",
        "def normalize_text(s):\n",
        "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "  import string, re\n",
        "\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "  def white_space_fix(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# Compute Exact Match Score (em score)\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "#Compute F1 score\n",
        "def compute_f1(prediction, truth):\n",
        "  # Normalizing predicted and true answer\n",
        "  pred_tokens = normalize_text(prediction).split()\n",
        "  truth_tokens = normalize_text(truth).split()\n",
        "  \n",
        "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "  \n",
        "  # Intersection of Predicted and true answer\n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "  \n",
        "  # if there are no common tokens then f1 = 0\n",
        "  if len(common_tokens) == 0:\n",
        "    return 0\n",
        "  \n",
        "  # Ratio of length of common tokens to complete tokens length\n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "  \n",
        "  # Return Harmonic Mean\n",
        "  return 2 * (prec * rec) / (prec + rec)\n",
        "\n",
        "# Predicting answer and returning metrics of evaluation \n",
        "def give_an_answer(context,query,answer):\n",
        "\n",
        "  prediction = predict(context,query)\n",
        "  em_score = compute_exact_match(prediction, answer)\n",
        "  f1_score = compute_f1(prediction, answer)\n",
        "\n",
        "  \n",
        "  print(f\"Question: {query}\")\n",
        "  print(f\"Prediction: {prediction}\")\n",
        "  print(f\"True Answer: {answer}\")\n",
        "  print(f\"EM: {em_score}\")\n",
        "  print(f\"F1: {f1_score}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return prediction\n",
        "\n",
        "st.title(\"Multilingual Question Answering System\")\n",
        "para = st.text_input(\"Enter Paragraph\")\n",
        "ques = st.text_input(\"Enter your Question\")\n",
        "\n",
        "\n",
        "context = str(para)\n",
        "queries = str(ques)\n",
        "\n",
        "answer = \"cricket\"\n",
        "\n",
        "\n",
        "if(st.button(\"Predict\")):\n",
        "    if(queries==\"\" or context==\"\"):\n",
        "      st.error(\"Fill the details Correctly\")\n",
        "    \n",
        "    else :\n",
        "      answer = give_an_answer(context, queries, answer)\n",
        "      st.success('Predicted answer is {}'.format(answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nU1K22IOfgkr"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owz5zxufgGNY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "UI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}