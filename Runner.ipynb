{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Runner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "P2IoPCF8dqSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "yudFOLK8d3pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szfQ6KMk2AZ0",
        "outputId": "33a9235c-f67b-4a20-9f6b-4d7813a48234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the bert tokenizer and load fined tuned model\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "model_name1 = \"xlm-roberta-base\"\n",
        "model_name2 = \"xlm-roberta-large\"\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "\n",
        "path1 = f\"/content/drive/MyDrive/QA_System/Datasets/XQUAD/xlm_roberta_base5.pt\"\n",
        "path2 = f\"/content/drive/MyDrive/QA_System/Datasets/XQUAD/xlm_roberta_large5.pt\"\n",
        "\n",
        "model1=torch.load(path1,map_location=torch.device('cpu'))\n",
        "model1.eval()\n",
        "\n",
        "model2=torch.load(path1,map_location=torch.device('cpu'))\n",
        "model2.eval()"
      ],
      "metadata": {
        "id": "3qUmEW1LdKG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict answer based on provided context and query\n",
        "def predict(context,query):\n",
        "\n",
        "  # Encode query and context \n",
        "  inputs = tokenizer1.encode_plus(query, context, padding=True, max_length= 512, truncation=True, add_special_tokens = True, return_tensors='pt')\n",
        "\n",
        "  # Generate output\n",
        "  outputs = model2(**inputs)\n",
        "\n",
        "  # Extract start and end index \n",
        "  answer_start = torch.argmax(outputs[0])  # get the most likely beginning of answer with the argmax of the score\n",
        "  answer_end = torch.argmax(outputs[1]) + 1 \n",
        "  \n",
        "  # Decode answer from tokenized form\n",
        "  answer = tokenizer1.convert_tokens_to_string(tokenizer1.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "\n",
        "  return answer\n",
        "\n",
        "# Normalize text -> Remove articles, spaces, punctuation marks and conversion to all upper cases to lower cases\n",
        "def normalize_text(s):\n",
        "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "  import string, re\n",
        "\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "  def white_space_fix(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# Compute Exact Match Score (em score)\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "#Compute F1 score\n",
        "def compute_f1(prediction, truth):\n",
        "  # Normalizing predicted and true answer\n",
        "  pred_tokens = normalize_text(prediction).split()\n",
        "  truth_tokens = normalize_text(truth).split()\n",
        "  \n",
        "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "  \n",
        "  # Intersection of Predicted and true answer\n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "  \n",
        "  # if there are no common tokens then f1 = 0\n",
        "  if len(common_tokens) == 0:\n",
        "    return 0\n",
        "  \n",
        "  # Ratio of length of common tokens to complete tokens length\n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "  \n",
        "  # Return Harmonic Mean\n",
        "  return 2 * (prec * rec) / (prec + rec)"
      ],
      "metadata": {
        "id": "VkT7pqk6dW9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting answer and returning metrics of evaluation \n",
        "def give_an_answer(context,query,answer):\n",
        "\n",
        "  prediction = predict(context,query)\n",
        "  em_score = compute_exact_match(prediction, answer)\n",
        "  f1_score = compute_f1(prediction, answer)\n",
        "\n",
        "  \n",
        "  print(f\"Question: {query}\")\n",
        "  print(f\"Prediction: {prediction}\")\n",
        "  print(f\"True Answer: {answer}\")\n",
        "  print(f\"EM: {em_score}\")\n",
        "  print(f\"F1: {f1_score}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "aDKJB6z_dZlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = '''Hi! Mein Name ist Alexa und ich bin 21 Jahre alt. Ich habe gelebt\n",
        "            in Peristeri von Athen , aber jetzt zog ich weiter in Kaisariani von Athen.'''\n",
        "\n",
        "queries = [\"Wie alt ist Alexa?\",\n",
        "           \"Wo hat Alexa fr√ºher gelebt?\"\n",
        "          ]\n",
        "answers = [\"21\",\n",
        "           \"Peristeri of Athens\"\n",
        "          ]\n",
        "\n",
        "for q,a in zip(queries,answers):\n",
        "  (give_an_answer(context,q,a))\n",
        "\n"
      ],
      "metadata": {
        "id": "0U1Gi5e3eb_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}